#! /usr/bin/python3
# pylint: disable=wrong-import-position, broad-except, too-few-public-methods
# VDA-523, https://pcplatform.atlassian.net/browse/VDA-523
'''
python3 t_dense_sparse.py -d -e t|x|.. 
 * for group run, launch from ../run_tests.py or run_illmn.py

The original VCFs generated by Kushal are at sparkdmz1:/data/scratch/kdatta1/illmn
$ lsof -ad3-999 -c rsyn  # see what rsyn is doing
@dhls-skx-0 pw4mingrut=pw4root; @dhls-skx-2 pw4mingrut=pw4sparkdmz1
$ lspci | egrep -i --color 'network|ethernet'
$ lshw -class network           # nice one!
$ ethtool -i eno1

Eexperiment History:
Old outputs @ C:\\Users\\mrutarx\\myprojects\\GenomicsDBPerfTest\\dev_scratch\\logs
    e-0_ssdefault. ss=10M?
    e-1_ss10k. variant-density_old_logs\\variant-density_170717-1620: default segment
    e-2_ss50029k-gdbv1. variant-density : segment size = [500, 1000, 3000, 6000, 9000],
            without calls verification
  @C:C:\\Users\\mrutarx\\myprojects\\GenomicsDBPerfTest\\dev_scratch\\logs\\
    e-3_ref_block. variant-density: segment size = [500, 1000, 3000, 6000, 9000],
            with calls verification
  # use TEST_NAME = "variant-density_noref" - different tdb_ws
    e-4_var_only_nostat. variant-density: segment size = [500, 1000, 3000, 6000, 9000],
        pos=[1, 10, 100, 1000, 10000, 50000] repeat=3
          # will capture iostat
    e-5_var_only. default
    e-6_ssd. run it
    e-7_zlib. IA zlib. default
    e-8_zlib_ssd: IA zlib + ssd. default
    e-9_zlib_ssd_ref_block: IA zlib + ssd on TEST_NAME=variant-density
  # full GenomicsDB 16 partitions each 1000 pos
    e-10_16_hdd_zlib_refblock
    e-11_16_ssd_zlib_refblock
  # JIRA-126, each partition 4 profiling: ssd_zlib, hdd, refblock | varonly
    g-20, 21, 22, 23 16 part, ssd, iazlib, refblock
    g-30, 31, 32, 33 8 part,  hdd, oszlib  refblock
    g-40, 31, 32, 33 44 part, ssd, iazlib, varonly
    g-50, 31, 32, 33 88 part, hdd, oszlib, varonly

To view nvme use 'nvme --list'
'''
import sys
import os
import os.path
import platform
import pickle
from pprint import pprint
import argparse
from collections import namedtuple
import logging
from time import localtime, strftime
from pygenomicsdblib.utils.common import get_my_logger
log_fn = "%s_%s" % (os.path.basename(__file__)[:-3], strftime("%y%m%d%H%M", localtime()))
logger = get_my_logger(log_fn, logging.DEBUG) if __name__ == '__main__' else logging.getLogger("tmp")
from pygenomicsdblib.utils.histogram import HistogramManager
root_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if not root_path in sys.path:
    sys.path.append(root_path)
from shared.genopp_process_utils import gen_cfg_json4density, launch_profiling
from shared.profiling_env import set_run_type, get_histogram_root, get_cfg_json_ws, is_test as env_is_test
from shared.density_pos_selector import get_intpos_builder

HOST_LIST = []
HISTOGRAM_1K = "1000_histogram"                 # histogram file for our 1000 genomics VCF
TEST_NAME = "illmn"                          # "variant-density"

LOADER_MPIRUN_4 = 4
LOADER_MPIRUN_16 = 16
QUERY_REPEAT = 3            # 3 is normal, for 10/24 demo we run once

DEFAULT_RUN_OPTIONS = {
    'loader' : {'share' : True, 'force_reload_first' : False, 'num_parallel' : 1},
    'querier' : {'produce_opt' : '--print-AC', 'repeat' : QUERY_REPEAT, 'seg_size' : [1000, 5000, 10000]}}

exp_num = lambda x: int(x.split('-')[1])
json_config = namedtuple('TestConfig', ["test_name", "callsets", "vid_mapping", "ref_geno", "device", "db_type", "array_start_pos", "num_parallel", "num_pos2gen", "query_attributes", "num_frag"])

class BaseConfig():
    test_name = None
    exp_options = None  #hdd/ssd; oszlib/iazlib;
    num_sample_list = None
    q_group_cfg = None
    num_pos2gen = None
    histogram_fn = None
    num_partitions = None

    def __init__(self):
        self.run_options = DEFAULT_RUN_OPTIONS.copy()

    def _get_config(self):
        assert self.test_name and self.exp_options and self.q_group_cfg and self.num_pos2gen and self.num_sample_list

    @staticmethod
    def get_array_start_pos(histogram_fn, num_partitions):
        assert histogram_fn and num_partitions
        histogram_fpath = os.path.join(get_histogram_root(), histogram_fn)
        assert os.path.isfile(histogram_fpath), "histogram file %s not found" % histogram_fpath
        return HistogramManager(histogram_fpath).calc_bin_begin_pos(num_partitions)

class G1KConfig(BaseConfig):
    ''' refactor from __group_test_cfg '''
    HISTOGRAM_1K = "1000_histogram"                 # histogram file for our 1000 genomics VCF
    test_name = "variant-density"
    vid_fn = "vid.json"
    exp_options = [(True, True, True), (False, False, True), (True, True, False), (False, False, False)]  
    #hdd/ssd; oszlib/iazlib; repo
    query_parallel = -1
    num_sample_list = [(0, 1000)]
    q_group_cfg = {10 : 1, 20 : 8, 30 : 16, 40 : 44, 50 : 88}
    num_pos2gen = [65, 1000]                  # ([2, 4] if my_is_test else [65, 1000])        # variant-density

    def _config_exp(self, exp_id, idx, device, zlib, repo):
        db_type = 'refblock' if repo else 'varonly'
        my_exp_name = "%s-%d_%d_%s_%s_%s" % (exp_id.split('-')[0], idx, self.num_partitions, "hdd" if device else 'ssd', 'oszlib' if zlib else 'iazlib', db_type)
        callsets_fn = "callsets.json" if repo else "callsets_1468_no_REF_block.json"
        run_options = self.run_options.copy()
        if not zlib:
            zpath = os.path.join(os.environ['HOME'], 'opt', 'zlib', 'lib')
            run_options['add_to_lib_path'] = [zpath]
            # run_options['add_to_lib_path'] = [ '/home/mingrutar/opt/zlib/lib']
        run_options['loader']['num_parallel'] = self.num_partitions
        array_start_pos = self.get_array_start_pos(self.HISTOGRAM_1K, self.num_partitions)
        # self.query_parallel used as seed, so HDD and SSD will get the same positions
        json_cfg_options = json_config(self.test_name, callsets_fn, self.vid_fn, \
            "Homo_sapiens_assembly19.fasta", 0 if device else 1, db_type, array_start_pos, self.query_parallel, self.num_pos2gen, None, self.num_sample_list[1])
        return json_cfg_options, run_options, my_exp_name

    def get_config(self, my_exp_id):
        self._get_config()
        intid = exp_num(my_exp_id)
        assert intid in self.q_group_cfg
        self.query_parallel = self.num_partitions = self.q_group_cfg[intid]
        return [self._config_exp(my_exp_id, i+intid, d, z, r) for i, (d, z, r) in enumerate(self.exp_options)]

class IllmnConfig(BaseConfig):
    NUM_PARTITION = 16
    HISTOGRAM_10K = "histogram.10000"               # histogram file for our 1000 genomics VCF

    test_name = "illmn"
    callsets_fn = "callsets_illmn_15000.json"
    vid_fn = "illmn_vid.json"
    IllmnQueryAttr = ["REF", "ALT", "QUAL", "GT"]
    DeviceOptions = [(0, True), (1, False), (2, False)]   # , 0-hdd, 1-ssd, 2-nvme, True-oszlib, False-iazlib
    query_parallel = None
    num_sample_list = None
    q_group_cfg = {}
    num_pos2gen = None
    exp_options = [(0, True), (1, False)]

    def __init__(self, myconfig):
        assert len(myconfig) > 5 
        super(IllmnConfig, self).__init__()
        self.num_partition = self.NUM_PARTITION
        self.run_options['loader']['num_parallel'] = myconfig[0]
        self.num_sample_list = myconfig[1]
        self.q_group_cfg = myconfig[2]
        self.num_pos2gen = myconfig[3]
        self.array_start_pos = self.get_array_start_pos(self.HISTOGRAM_10K, self.NUM_PARTITION)
        self.run_options['loader']['force_reload_first'] = myconfig[4]
        if len(myconfig) >= 7:
            self.run_options['querier']['produce_opt'] = myconfig[5]
            self.run_options['querier']['repeat'] = myconfig[6]
            sel_list = myconfig[7] if len(myconfig) == 8 else [0, 1]
            self.exp_options = [self.DeviceOptions[i] for i in sel_list]

    def _config_exp(self, myid, nf_samples, device, zlib):
        my_exp_name = "%s_%d-%d_%s_%s_%d_%d" % (myid, self.num_partition, self.query_parallel, "hdd" if device else 'ssd', 'oszlib' if zlib else 'iazlib', nf_samples[0], nf_samples[1])
        my_run_options = self.run_options.copy()
        if not zlib:
            zpath = os.path.join(os.environ['HOME'], 'opt', 'zlib', 'lib')
            my_run_options['add_to_lib_path'] = [zpath]
            # my_run_options['add_to_lib_path'] = ['/home/mingrutar/opt/zlib/lib']
        json_cfg_options = json_config(self.test_name, self.callsets_fn, self.vid_fn, \
            "Homo_sapiens_assembly19.fasta", device, "varonly", self.array_start_pos, self.query_parallel, self.num_pos2gen, self.IllmnQueryAttr, nf_samples)
        return json_cfg_options, my_run_options, my_exp_name

    def get_config(self, my_exp_id, num_sample_list=None):
        n_samples = num_sample_list if num_sample_list else self.num_sample_list
        assert self.num_sample_list and self.num_pos2gen and self.q_group_cfg
        intid = exp_num(my_exp_id)
        assert intid in self.q_group_cfg

        self.query_parallel = self.q_group_cfg[intid]
        cfg_list = [self._config_exp("%s-%d" % (my_exp_id.split('-')[0], intid + (i + (2 * j))), n, d, z)  \
        for i, (d, z) in enumerate(self.exp_options) for j, n in enumerate(n_samples)]
        return cfg_list

def alter_test_cfg(exp_id, my_is_test):
    ''' BACKWARD compatible for old exp alter test config as described at head
    # e-7: zlib, [500, 1000, 3000, 6000, 9000], 'add_to_lib_path':  ['/home/mingrutar/opt/zlib/lib']
    '''
    name_parts = []
    nid = int(exp_id.split('-')[1])
    if exp_id in ['e-3', 'e-9', 'e-10', 'e-11']:
        name_parts.append('refblock')
        test_name = TEST_NAME
        callsets_fn = "callsets.json"
    else:
        name_parts.append('varonly')
        test_name = "%s_noref" % TEST_NAME
        callsets_fn = "callsets_1468_no_REF_block.json"

    run_options = DEFAULT_RUN_OPTIONS.copy()
    if exp_id in ['e-7', 'e-8', 'e-9', 'e-10', 'e-11']:
        zpath = os.path.join(os.environ['HOME'], 'opt', 'zlib', 'lib')
        run_options['add_to_lib_path'] = [zpath]
        # run_options['add_to_lib_path'] = ['/home/mingrutar/opt/zlib/lib']
        name_parts.append('zlib')
    is_ssd = 1 if exp_id in ['e-6', 'e-8', 'e-9', 'e-11'] else 0
    name_parts.append('ssd' if is_ssd else 'hdd')
    num_partitions = 1 if nid < 10 else 16
    num_pos2gen = ([2, 4] if my_is_test else [65, 1000]) if nid >= 10 else \
     ([10, 100] if my_is_test else [1, 10, 100, 1000])   # 65x16 =1040, 1000 * 16
    name_parts.append(str(num_partitions))
    name_parts.reverse()

    num_parallel = num_partitions
    histogram_fpath = os.path.join(get_histogram_root(), HISTOGRAM_1K)
    assert os.path.isfile(histogram_fpath), "histogram file %s not found" % histogram_fpath
    array_start_pos = HistogramManager(histogram_fpath).calc_bin_begin_pos(num_partitions)
    JSONConfig = namedtuple('TestConfig', ["test_name", "callsets", "vid_mapping", \
        "ref_geno", "device", "array_start_pos", "num_parallel", "num_pos2gen", "db_type"])
    json_cfg_options = JSONConfig(test_name, callsets_fn, "vid.json", \
        "Homo_sapiens_assembly19.fasta", is_ssd, array_start_pos, num_parallel, num_pos2gen)
    return json_cfg_options, run_options, "%s_%s" % (exp_id, '_'.join(name_parts))

def __save_data(test_name, fn, mydata):
    out2_fn = os.path.join(get_cfg_json_ws(), test_name, fn)
    with open(out2_fn, 'wb') as ofd:
        pickle.dump(mydata, ofd)
    return out2_fn

def run_test_old(proj_name, my_exp_id, my_is_test, my_dryrun, host_list=None):
    ''' for pre group tests, hope works '''
    set_run_type(my_is_test)                           #True for production
    print("INFO: proj_name=%s, test_id=%s is_test=%s, dryrun=%s, env.is_test=%s" % \
        (proj_name, my_exp_id, my_is_test, my_dryrun, env_is_test()))
    # if exp_num(my_exp_id) not in group_partitions:
    #     return _run_test(my_exp_id, my_reload_tdb, my_is_test, my_dryrun)
    # [(json_cfg, run_opts, my_exp_name)]
    cfg_list = [alter_test_cfg(my_exp_id, my_is_test)]
    pos_selector = get_intpos_builder(proj_name)
    exp_name_list = [cfg[2] for cfg in cfg_list]
    run_group, sel_pos_group = {}, {}
    for json_cfg, run_opts, my_exp_name in cfg_list:
        print("INFO: exp_name=%s, json_cfg=%s, run_opts=%s" % (my_exp_name, json_cfg, run_opts))
        run_list, sel_pos = gen_cfg_json4density(my_exp_name, my_is_test, pos_selector, json_cfg, run_opts['loader']['share'])
        if run_list:
            run_group[my_exp_name] = {'run_options' : run_opts, 'configs' : run_list}
        if sel_pos:
            sel_pos_group[my_exp_name] = sel_pos
    if sel_pos_group:
        __save_data(proj_name, 'sel_pos-%s.info' % my_exp_id, sel_pos_group)
    if run_group:
        my_host_list = host_list if host_list else HOST_LIST
        run_list_and_options = __save_data(proj_name, 'run_list-%s' % my_exp_id, run_group)
        print("INFO t_dense_sparse: run_list_and_options=%s, host_list=%s" % \
            (run_list_and_options, my_host_list))
        if not my_dryrun:          # not dryrun
            launch_profiling(run_list_and_options, my_host_list, exp_name_list[0]) #nohug.* in first exp
            print("INFO: Launched %s" % exp_name_list)
        else:
            with open(run_list_and_options, 'rb') as ifd:
                pprint(pickle.load(ifd))
            print("INFO: Done dry_run %s" % exp_name_list)
    else:
        print("WARN: no config json files were generated")
    return exp_name_list

gid_parallel_o = {10 : 16}                  # 16 partitions 
gid_parallel_4 = {10 : 1, 20 : 4}           # canot load 16 partitions 
gid_parallel_16 = {10 : 1, 20 : 4, 30 : 8, 40 : 16}
gid_parallel_88 = {10 : 32, 20 : 44, 30 : 88} 
IllmnConfigs = {  # (loader_parallel, sample_list, gid:parallel, num_pos_list, force_reload_first, [query_option, number_repeat], [None=both|True=SSD-only|False=HDD-only])
    'i' : (LOADER_MPIRUN_4, [(0, 5000), (0, 10000), (0, 15000)], gid_parallel_4, [250, 1000], False),
    'a' : (LOADER_MPIRUN_4, [(14, 1000), (4, 3000), (1, 7500), (0, 15000)], gid_parallel_4, [250, 1000], True, '--print-AC', 1),

    'x' : (LOADER_MPIRUN_16, [(0, 10000)], gid_parallel_16, [250, 1000], False, '--print-calls', 3),
    'y' : (LOADER_MPIRUN_16, [(0, 10000)], gid_parallel_16, [250, 1000], False, '--print-calls', 1),
    # 15K sample
    'c' : (LOADER_MPIRUN_16, [(0, 10000)], gid_parallel_16, [250, 1000], False, '--print-AC', 1),
    #  SSD only, 
    'd' : (LOADER_MPIRUN_16, [(0, 10000)], gid_parallel_88, [250, 1000], False, '--print-AC', 1, [1]),
    # 10K samples
    'g' : (LOADER_MPIRUN_16, [(0, 10000)], gid_parallel_16, [250, 1000], False, '--print-AC', 1),
    # SSD optane, 15K samples
    'o' : (LOADER_MPIRUN_16, [(0, 15000)], gid_parallel_o, [1000], False, '--print-AC', 1, [2]),
    # HDD only 15K samples on dhls-skx-2
    'p' : (LOADER_MPIRUN_16, [(0, 15000)], gid_parallel_o, [1000], False, '--print-AC', 1, [0]),

    's' : (LOADER_MPIRUN_16, [(0, 15000)], gid_parallel_o, [10], False, '--print-AC', 1, [0]),
    't' : (LOADER_MPIRUN_16, [(0, 15000)], gid_parallel_o, [10], False, '--print-AC', 1, [2]),
    'u' : (LOADER_MPIRUN_16, [(0, 15000)], gid_parallel_o, [10], False, '--print-AC', 1, [0])}

MyIllmnConfig = lambda cfg_prefix: IllmnConfig(IllmnConfigs[cfg_prefix])
def generate_jsons(proj_name, cfg_prefix, my_is_test, exp_list=None):
    pos_selector = get_intpos_builder(proj_name)
    cfg = G1KConfig() if cfg_prefix == 'e' else MyIllmnConfig(cfg_prefix)
    myexp_list = exp_list if exp_list else cfg.q_group_cfg.keys()
    all_runs = []
    for exp_id in myexp_list:
        my_exp_id = "%s-%s" % (cfg_prefix, exp_id)
        cfg_list = cfg.get_config(my_exp_id)
        run_group, sel_pos_group = {}, {}
        for json_cfg, run_opts, my_exp_name in cfg_list:
            print("INFO: exp_name=%s, json_cfg=%s, run_opts=%s" % (my_exp_name, json_cfg, run_opts))
            run_list, sel_pos_list = gen_cfg_json4density(my_exp_name, my_is_test, pos_selector, json_cfg, run_opts['loader']['share'])
            if run_list:
                run_group[my_exp_name] = {'run_options' : run_opts, 'configs' : run_list}
            if sel_pos_list:
                sel_pos_group[my_exp_name] = sel_pos_list
        if sel_pos_group:
            __save_data(proj_name, 'sel_pos-%s.info' % my_exp_id, sel_pos_group)
        if run_group:
            run_list_fn = __save_data(proj_name, 'run_list-%s' % my_exp_id, run_group)
            all_runs.append(run_list_fn)        #[run_list-i-10,.. run_list-i-30]
    return all_runs

def show_runlist(all_runs):
    print("\nINFO: number of run_list file is ", len(all_runs))
    for rfn in all_runs:
        print("\nINFO: dump run_list file", rfn)
        with open(rfn, 'rb') as ifd:
            pprint(pickle.load(ifd))

def run_all_tests(proj_name, cfg_prefix, my_dryrun, my_is_test=False, host_list=None):
    ''' cfg_prefix = i (single frag) or a (multi frag) '''
    set_run_type(my_is_test)                           #True for production
    all_runs = generate_jsons(proj_name, cfg_prefix, my_is_test)
    if my_dryrun:
        show_runlist(all_runs)
        print("INFO: Done dry_run %s, %s" % (proj_name, cfg_prefix))
    else:
        my_host_list = host_list if host_list else HOST_LIST
        print("INFO t_dense_sparse: #run_list_files=%ds, host_list=%s" % (len(all_runs), my_host_list))
    #     ???
    #     launch_profiling(all_runs, my_host_list, exp_name_list[0]) #nohug.* in first exp
    #     print("INFO: Launched %s, %s" % (proj_name, cfg_prefix)
    return all_runs

if __name__ == '__main__':
    my_parser = argparse.ArgumentParser()
    my_parser.add_argument('-e', type=str, nargs='?', const=True, default='t', help='eid')
    my_parser.add_argument('-t', type=bool, nargs='?', const=True, default=False, help='run as test')
    my_parser.add_argument('-d', type=bool, nargs='?', const=True, default=True, help='dryrun')
    myargs = my_parser.parse_args()
    dryrun = True if platform.system() == 'Windows' else myargs.d

    # run_flist = run_all_tests(TEST_NAME, 'a', dryrun, myargs.t)
    run_flist = run_all_tests(TEST_NAME, myargs.e, dryrun, myargs.t)
    print("INFO: MAIN Completed, runlist_files are:")
    pprint(run_flist)

# def __illmn_group_cfg(my_exp_id, num_sample_list):
#     ''' generate a group of exp'''
#     q_group_cfg = {10 : 1, 20 : 4, 30 : 8, 40 : 16}       # 1 or 4 parallel
#     exp_options = [(True, True), (False, False)]  #hdd/ssd; oszlib/iazlib;
#     intid = exp_num(my_exp_id)
#     assert intid in q_group_cfg
#     num_partition = 16
#     # histogram_fn = "histogram.5000" if num_samples == 50000 else "histogram.10000"
#     histogram_fn = "histogram.10000"
#     histogram_fpath = os.path.join(get_histogram_root(), histogram_fn)
#     array_start_pos = HistogramManager(histogram_fpath).calc_bin_begin_pos(num_partition)
#     num_parallel = q_group_cfg[intid]
#     num_pos2gen = ([2, 4] if platform.system() == 'Windows' else [250, 1000])
#     IllmnQueryAttr = ["REF", "ALT", "QUAL", "GT"]
#     jsonconfig = namedtuple('TestConfig', ["test_name", "callsets", "vid_mapping", \
#         "ref_geno", "is_ssd", "db_type", "array_start_pos", "num_parallel", "num_pos2gen", "query_attributes"])

#     def __config_exp(idx, num_samples, device, zlib):
#         callsets_fn = "callsets_illmn_%d.json" % num_samples
#         assert os.path.isfile(histogram_fpath), "histogram file %s not found" % histogram_fpath
#         my_exp_name = "%s-%d_%d-%d_%s_%s_%dk" % (my_exp_id.split('-')[0], intid+idx, num_partition, num_parallel, "hdd" if device else 'ssd', 'oszlib' if zlib else 'iazlib', num_samples if num_samples < 1000 else int(num_samples/1000))
#         run_options = DEFAULT_RUN_OPTIONS.copy()
#         if not zlib:
#             run_options['add_to_lib_path'] = ['$HOME/opt/zlib/lib']
#         run_options['loader']['num_parallel'] = num_partition if lnum_parallel_as_partition else num_parallel
#         json_cfg_options = jsonconfig(TEST_NAME, callsets_fn, "illmn_vid.json", \
#             "Homo_sapiens_assembly19.fasta", not device, "varonly", array_start_pos, num_parallel, num_pos2gen, IllmnQueryAttr)
#         return json_cfg_options, run_options, my_exp_name
#     cfg_list = [__config_exp(i+(2*j), n, d, z) for i, (d, z) in enumerate(exp_options) \
#                 for j, n in enumerate(num_sample_list)]
#     return cfg_list

# def __group_test_cfg(my_exp_id, my_is_test):
#     ''' generate a group of exp'''
#     group_partitions = {10 : 1, 20 : 8, 30 : 16, 40 : 44, 50 : 88}   # 88 not working
#     exp_options = [(True, True, True), (False, False, True), (True, True, False), (False, False, False)]  #hdd/ssd; oszlib/iazlib; refblock/varonly
#     intid = exp_num(my_exp_id)
#     assert intid in group_partitions

#     histogram_fpath = os.path.join(get_histogram_root(), HISTOGRAM_1K)
#     assert os.path.isfile(histogram_fpath), "histogram file %s not found" % histogram_fpath
#     num_partitions = group_partitions[intid]
#     num_parallel = num_partitions
#     num_pos2gen = ([2, 4] if my_is_test else [65, 1000])
#     jsonconfig = namedtuple('TestConfig', ["test_name", "callsets", "vid_mapping", \
#         "ref_geno", "is_ssd", "db_type", "array_start_pos", "num_parallel", "num_pos2gen"])
#     def __config_exp(idx, device, zlib, repo):
#         db_type = 'refblock' if repo else 'varonly'
#         my_exp_name = "%s-%d_%d_%s_%s_%s" % (my_exp_id.split('-')[0], intid+idx, num_partitions, "hdd" if device else 'ssd', \
#             'oszlib' if zlib else 'iazlib', db_type)
#         callsets_fn = "callsets.json" if repo else "callsets_1468_no_REF_block.json"
#         run_options = DEFAULT_RUN_OPTIONS.copy()
#         if not zlib:
#             run_options['add_to_lib_path'] = ['$HOME/opt/zlib/lib']
#         run_options['loader']['num_parallel'] = num_partitions
#         array_start_pos = HistogramManager(histogram_fpath).calc_bin_begin_pos(num_partitions)
#         json_cfg_options = jsonconfig(TEST_NAME, callsets_fn, "vid.json", \
#             "Homo_sapiens_assembly19.fasta", not device, db_type, array_start_pos, num_parallel, num_pos2gen)
#         return json_cfg_options, run_options, my_exp_name
#     return [__config_exp(i, d, z, r) for i, (d, z, r) in enumerate(exp_options)]

# def __generate_run(test_name, my_exp_id, run_list):
#     ''' used only in run_test_old prepare for run and lauch execution at remote '''
#     run_list_fn = __save_data(test_name, 'run_list-%s' % my_exp_id, run_list)
#     try:
#         linked_fn = os.path.join(os.path.dirname(run_list_fn), os.path.basename(__file__))
#         if not os.path.exists(linked_fn):
#             os.symlink(os.path.abspath(__file__), linked_fn)
#     except Exception:
#         print("INFO failed to create symbol link, ignored")
#     return run_list_fn
